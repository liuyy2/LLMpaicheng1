"""
LLM å…ƒå‚æ•°ç­–ç•?- ä½¿ç”¨ LLMï¼ˆæˆ–æ¨¡æ‹Ÿ LLMï¼‰è¾“å‡ºå…ƒå‚æ•° JSON

ç‰¹ç‚¹ï¼?
- LLM åªè¾“å‡ºå…ƒå‚æ•° JSONï¼ˆè§¦å‘é˜ˆå€?å†»ç»“æ—¶é•¿/æƒ©ç½šæƒé‡ï¼?
- CP-SAT ä»ç„¶æ˜¯å”¯ä¸€çš„æ’ç¨‹ä¼˜åŒ–å™¨
- æä¾› JSON schema æ ¡éªŒï¼ˆä¸‰å±‚æŠ½å–ï¼‰
- æ ¡éªŒå¤±è´¥æ—?fallback åˆ°å›ºå®šç­–ç•?
- æ”¯æŒçœŸå® LLM æ¥å£ï¼ˆQwen3-32B via ModelScopeï¼?

ç­–ç•¥ç±»å‹ï¼?
- MockLLMPolicy: æ ¹æ®ç‰¹å¾ç¡®å®šæ€§è¾“å‡?JSONï¼ˆæ¸©åº?0 ç­‰ä»·ï¼Œç”¨äºç¦»çº¿å¤ç°ï¼‰
- RealLLMPolicy: çœŸå® LLM è°ƒç”¨æ¥å£ï¼ˆè°ƒç”?Qwen3-32Bï¼?

ä¸¥ç¦ï¼šLLM ç›´æ¥è¾“å‡ºä»»åŠ¡çº§æ’ç¨?
"""

import json
import os
from typing import Optional, Tuple, Dict, Any, List, Callable, Set
from dataclasses import dataclass, field, asdict
from datetime import datetime

# LLM Client
try:
    from llm_client import LLMClient, LLMConfig, LLMCallResult, extract_json_from_text
    HAS_LLM_CLIENT = True
except ImportError:
    HAS_LLM_CLIENT = False

from policies.base import BasePolicy, MetaParams
from disturbance import SimulationState
from config import Config
from solver_cpsat import Plan
from features import compute_state_features, compute_state_features_ops, StateFeatures


def _compute_state_features_for_policy(
    state: SimulationState,
    now: int,
    config: Config,
    prev_window_slots: Optional[Dict[str, Set[int]]],
    recent_shifts: int,
    recent_switches: int
) -> Tuple[StateFeatures, Dict[str, Set[int]]]:
    if hasattr(state, 'missions') and hasattr(state, 'resources'):
        return compute_state_features_ops(
            missions=state.missions,
            resources=state.resources,
            current_plan=state.current_plan,
            now=now,
            config=config,
            completed_ops=getattr(state, 'completed_ops', set()),
            prev_window_slots=prev_window_slots,
            recent_shifts=recent_shifts,
            recent_switches=recent_switches
        )

    return compute_state_features(
        tasks=state.tasks,
        pads=state.pads,
        current_plan=state.current_plan,
        now=now,
        config=config,
        completed_tasks=state.completed_tasks,
        prev_window_slots=prev_window_slots,
        recent_shifts=recent_shifts,
        recent_switches=recent_switches
    )



# ============================================================================
# JSON Schema å®šä¹‰
# ============================================================================

META_PARAMS_SCHEMA: Dict[str, Any] = {
    "type": "object",
    "required": ["w_delay", "w_shift", "w_switch"],
    "properties": {
        "w_delay": {
            "type": "number",
            "minimum": 0.1,
            "maximum": 100.0,
            "default": 10.0,
            "description": "å»¶è¿Ÿæƒ©ç½šæƒé‡"
        },
        "w_shift": {
            "type": "number",
            "minimum": 0.0,
            "maximum": 50.0,
            "default": 1.0,
            "description": "æ—¶é—´åç§»æƒ©ç½šæƒé‡"
        },
        "w_switch": {
            "type": "number",
            "minimum": 0.0,
            "maximum": 300.0,
            "default": 5.0,
            "description": "Pad åˆ‡æ¢æƒ©ç½šæƒé‡"
        },
        "freeze_horizon": {
            "type": "integer",
            "minimum": 0,
            "maximum": 72,
            "default": 12,
            "description": "å†»ç»“è§†é‡ï¼ˆslotsï¼?
        }
    }
}

# é»˜è®¤å€?
DEFAULT_META_PARAMS: Dict[str, Any] = {
    "w_delay": 10.0,
    "w_shift": 1.0,
    "w_switch": 5.0,
    "freeze_horizon": 12
}


# ============================================================================
# JSON æ ¡éªŒä¸è§£æ?
# ============================================================================

@dataclass
class ValidationResult:
    """æ ¡éªŒç»“æœ"""
    is_valid: bool
    params: Optional[Dict[str, Any]] = None
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    extraction_method: str = ""  # "direct" | "code_fence" | "brace_search" | "failed"


def _simple_json_extract(text: str) -> Tuple[Optional[str], str]:
    """
    ç®€å?JSON æŠ½å–ï¼ˆå½“ llm_client ä¸å¯ç”¨æ—¶çš„å›é€€ï¼?
    
    ä¸‰å±‚æŠ½å–ï¼?
    1. ç›´æ¥å°è¯•ï¼ˆæ•´ä¸ªæ–‡æœ¬å°±æ˜?JSONï¼?
    2. Code fence æŠ½å–
    3. Brace æœç´¢
    """
    import re
    
    if not text:
        return None, "failed"
    
    text = text.strip()
    
    # å±‚çº§ 1: ç›´æ¥å°è¯•
    if text.startswith('{') and text.endswith('}'):
        try:
            json.loads(text)
            return text, "direct"
        except json.JSONDecodeError:
            pass
    
    # å±‚çº§ 2: Code fence
    patterns = [r'```json\s*([\s\S]*?)\s*```', r'```\s*([\s\S]*?)\s*```']
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            content = match.group(1).strip()
            if content.startswith('{'):
                try:
                    json.loads(content)
                    return content, "code_fence"
                except json.JSONDecodeError:
                    pass
    
    # å±‚çº§ 3: Brace searchï¼ˆå¤„ç†åµŒå¥—ï¼‰
    start = text.find('{')
    if start != -1:
        depth = 0
        in_string = False
        escape = False
        for i, c in enumerate(text[start:], start):
            if escape:
                escape = False
                continue
            if c == '\\' and in_string:
                escape = True
                continue
            if c == '"' and not escape:
                in_string = not in_string
                continue
            if in_string:
                continue
            if c == '{':
                depth += 1
            elif c == '}':
                depth -= 1
                if depth == 0:
                    try:
                        json.loads(text[start:i+1])
                        return text[start:i+1], "brace_search"
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None, "failed"


def validate_meta_params_json(
    raw_text: str,
    schema: Dict[str, Any] = META_PARAMS_SCHEMA
) -> ValidationResult:
    """
    æ ¡éªŒå…ƒå‚æ•?JSONï¼ˆä¸‰å±‚æŠ½å?+ Schema æ ¡éªŒï¼?
    
    æ ¡éªŒæµç¨‹ï¼?
    1. ä¸‰å±‚ JSON æŠ½å–ï¼ˆdirect / code fence / brace searchï¼?
    2. å¿…éœ€å­—æ®µå­˜åœ¨æ€§æ£€æŸ?
    3. å­—æ®µç±»å‹æ£€æŸ?
    4. å€¼èŒƒå›´æ£€æŸ¥ï¼ˆè¶…å‡ºèŒƒå›´åˆ™æˆªæ–­å¹¶è­¦å‘Šï¼?
    5. ç¼ºå¤±å­—æ®µè¡¥é»˜è®¤å€?
    
    Args:
        raw_text: åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å?code fence æˆ–å…¶ä»–å†…å®¹ï¼‰
        schema: JSON Schema
    
    Returns:
        ValidationResult
    """
    errors: List[str] = []
    warnings: List[str] = []
    extraction_method = "failed"
    
    # 1. ä¸‰å±‚ JSON æŠ½å–
    if HAS_LLM_CLIENT:
        json_str, extraction_method = extract_json_from_text(raw_text)
    else:
        json_str, extraction_method = _simple_json_extract(raw_text)
    
    if json_str is None:
        return ValidationResult(
            is_valid=False,
            errors=[f"æ— æ³•ä»æ–‡æœ¬ä¸­æŠ½å– JSON: {raw_text[:100]}..."],
            extraction_method=extraction_method
        )
    
    # 2. è§£æ JSON
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError as e:
        return ValidationResult(
            is_valid=False,
            errors=[f"JSON è§£æå¤±è´¥: {e}"],
            extraction_method=extraction_method
        )
    
    if not isinstance(data, dict):
        return ValidationResult(
            is_valid=False,
            errors=["JSON æ ¹å…ƒç´ å¿…é¡»æ˜¯å¯¹è±¡"],
            extraction_method=extraction_method
        )
    
    # 3. æ£€æŸ¥å¿…éœ€å­—æ®µ
    required = schema.get("required", [])
    for field_name in required:
        if field_name not in data:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field_name}")
    
    # 4. æ ¡éªŒæ¯ä¸ªå­—æ®µï¼ˆç±»å?+ èŒƒå›´ï¼?
    props = schema.get("properties", {})
    validated_params: Dict[str, Any] = {}
    
    for field_name, field_schema in props.items():
        if field_name in data:
            value = data[field_name]
            field_type = field_schema.get("type")
            
            # ç±»å‹æ£€æŸ¥ä¸è½¬æ¢
            if field_type == "number":
                if not isinstance(value, (int, float)):
                    errors.append(f"{field_name} å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ï¼Œgot {type(value).__name__}")
                    continue
                value = float(value)
            elif field_type == "integer":
                if isinstance(value, float) and value.is_integer():
                    value = int(value)
                    warnings.append(f"{field_name} å·²ä» float è½¬æ¢ä¸?integer")
                elif not isinstance(value, int):
                    errors.append(f"{field_name} å¿…é¡»æ˜¯æ•´æ•°ç±»å‹ï¼Œgot {type(value).__name__}")
                    continue
            
            # èŒƒå›´æ£€æŸ¥ï¼ˆè¶…å‡ºåˆ™æˆªæ–­ï¼‰
            min_val = field_schema.get("minimum")
            max_val = field_schema.get("maximum")
            
            if min_val is not None and value < min_val:
                warnings.append(f"{field_name}={value} < min={min_val}ï¼Œå·²æˆªæ–­")
                value = min_val
            
            if max_val is not None and value > max_val:
                warnings.append(f"{field_name}={value} > max={max_val}ï¼Œå·²æˆªæ–­")
                value = max_val
            
            validated_params[field_name] = value
        else:
            # ä½¿ç”¨é»˜è®¤å€?
            default = field_schema.get("default")
            if default is not None:
                validated_params[field_name] = default
                warnings.append(f"{field_name} ä½¿ç”¨é»˜è®¤å€?{default}")
    
    # å¦‚æœæœ‰ä¸¥é‡é”™è¯¯ï¼ˆå¿…éœ€å­—æ®µç¼ºå¤±æˆ–ç±»å‹é”™è¯¯ï¼‰ï¼Œè¿”å›å¤±è´?
    if errors:
        return ValidationResult(
            is_valid=False,
            errors=errors,
            warnings=warnings,
            extraction_method=extraction_method
        )
    
    return ValidationResult(
        is_valid=True,
        params=validated_params,
        warnings=warnings,
        extraction_method=extraction_method
    )


def json_to_meta_params(validated_params: Dict[str, Any]) -> MetaParams:
    """å°†æ ¡éªŒåçš„å‚æ•°è½¬ä¸?MetaParams"""
    return MetaParams(
        w_delay=validated_params.get("w_delay", DEFAULT_META_PARAMS["w_delay"]),
        w_shift=validated_params.get("w_shift", DEFAULT_META_PARAMS["w_shift"]),
        w_switch=validated_params.get("w_switch", DEFAULT_META_PARAMS["w_switch"]),
        freeze_horizon=validated_params.get("freeze_horizon", DEFAULT_META_PARAMS["freeze_horizon"])
    )


# ============================================================================
# LLM ç­–ç•¥æ—¥å¿—
# ============================================================================

@dataclass
class LLMDecisionLog:
    """å•æ¬¡ LLM å†³ç­–æ—¥å¿—"""
    timestamp: str
    episode_id: str
    t_now: int
    features: Dict[str, Any]
    
    # LLM è°ƒç”¨ä¿¡æ¯
    llm_cache_hit: bool
    llm_latency_ms: int
    usage_tokens: int
    
    # è§£æä¿¡æ¯
    raw_output: str
    extraction_method: str
    parsed_json: Optional[Dict[str, Any]]
    parsed_ok: bool
    validation_errors: List[str]
    validation_warnings: List[str]
    
    # Fallback ä¿¡æ¯
    fallback_used: bool
    fallback_reason: Optional[str]
    
    # æœ€ç»ˆå‚æ•?
    final_params: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class LLMPolicyLogger:
    """LLM ç­–ç•¥æ—¥å¿—è®°å½•å™?""
    
    def __init__(self, log_dir: Optional[str] = None, episode_id: str = ""):
        self.log_dir = log_dir
        self.episode_id = episode_id
        self.logs: List[LLMDecisionLog] = []
    
    def set_episode_id(self, episode_id: str) -> None:
        """è®¾ç½® episode ID"""
        self.episode_id = episode_id
    
    def log_decision(
        self,
        t_now: int,
        features: StateFeatures,
        raw_output: str,
        extraction_method: str,
        validation_result: ValidationResult,
        llm_cache_hit: bool,
        llm_latency_ms: int,
        usage_tokens: int,
        fallback_used: bool,
        fallback_reason: Optional[str],
        final_params: MetaParams
    ) -> LLMDecisionLog:
        """è®°å½•ä¸€æ¬¡å†³ç­?""
        log = LLMDecisionLog(
            timestamp=datetime.now().isoformat(),
            episode_id=self.episode_id,
            t_now=t_now,
            features=features.to_dict(),
            llm_cache_hit=llm_cache_hit,
            llm_latency_ms=llm_latency_ms,
            usage_tokens=usage_tokens,
            raw_output=raw_output,
            extraction_method=extraction_method,
            parsed_json=validation_result.params,
            parsed_ok=validation_result.is_valid,
            validation_errors=validation_result.errors,
            validation_warnings=validation_result.warnings,
            fallback_used=fallback_used,
            fallback_reason=fallback_reason,
            final_params={
                "w_delay": final_params.w_delay,
                "w_shift": final_params.w_shift,
                "w_switch": final_params.w_switch,
                "freeze_horizon": final_params.freeze_horizon
            }
        )
        self.logs.append(log)
        return log
    
    def save_logs(self, filepath: str) -> None:
        """ä¿å­˜æ—¥å¿—åˆ?JSONL æ–‡ä»¶"""
        if not self.logs:
            return
        
        os.makedirs(os.path.dirname(filepath) or ".", exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for log in self.logs:
                f.write(json.dumps(log.to_dict(), ensure_ascii=False) + '\n')
    
    def append_log(self, filepath: str, log: LLMDecisionLog) -> None:
        """è¿½åŠ å•æ¡æ—¥å¿—åˆ°æ–‡ä»?""
        os.makedirs(os.path.dirname(filepath) or ".", exist_ok=True)
        
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log.to_dict(), ensure_ascii=False) + '\n')
    
    def clear(self) -> None:
        """æ¸…ç©ºæ—¥å¿—"""
        self.logs = []
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡"""
        if not self.logs:
            return {"total": 0}
        
        cache_hits = sum(1 for log in self.logs if log.llm_cache_hit)
        fallbacks = sum(1 for log in self.logs if log.fallback_used)
        parse_fails = sum(1 for log in self.logs if not log.parsed_ok)
        total_tokens = sum(log.usage_tokens for log in self.logs)
        total_latency = sum(log.llm_latency_ms for log in self.logs)
        
        return {
            "total": len(self.logs),
            "cache_hits": cache_hits,
            "fallbacks": fallbacks,
            "parse_failures": parse_fails,
            "total_tokens": total_tokens,
            "total_latency_ms": total_latency,
            "cache_hit_rate": cache_hits / len(self.logs) if self.logs else 0,
            "fallback_rate": fallbacks / len(self.logs) if self.logs else 0
        }


# ============================================================================
# Prompt æ„å»º
# ============================================================================

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªç«ç®­å‘å°„æ’ç¨‹ä¼˜åŒ–çš„â€œå…ƒå‚æ•°è°ƒå‚å™¨â€ã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼šåœ¨ä¸æ˜¾è‘—å¢åŠ å»¶è¿Ÿçš„å‰æä¸‹ï¼Œå°½é‡é™ä½è®¡åˆ’æ¼‚ç§»ï¼ˆdriftï¼‰ã€?

é‡è¦è§„åˆ™ï¼?
1. ä½ åªèƒ½è¾“å‡ºå…ƒå‚æ•° JSONï¼Œä¸¥ç¦è¾“å‡ºå…·ä½“çš„ä»»åŠ¡æ’ç¨‹
2. å‚æ•°å°†ä¼ é€’ç»™ CP-SAT ä¼˜åŒ–å™¨è¿›è¡Œå®é™…æ’ç¨?
3. åªè¾“å‡?JSONï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦ä»£ç å?

å½“å‰è°ƒå‚åŸºçº¿ï¼ˆæ— å¼ºä¿¡å·æ—¶ç›´æ¥è¿”å›ï¼‰ï¼š
{"w_delay": 50.0, "w_shift": 0.0, "w_switch": 180.0, "freeze_horizon": 0}

å»ºè®®è¾“å‡ºèŒƒå›´ï¼ˆä¼˜å…ˆåœ¨æ­¤èŒƒå›´å†…å¾®è°ƒï¼‰ï¼š
- w_delay: 20.0 - 80.0
- w_shift: 0.0 - 8.0
- w_switch: 60.0 - 240.0
- freeze_horizon: 0 - 24

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ?JSONï¼‰ï¼š
{"w_delay": <number>, "w_shift": <number>, "w_switch": <number>, "freeze_horizon": <int>}"""


def build_user_prompt(features: StateFeatures) -> str:
    """æ„å»ºç”¨æˆ· Prompt"""
    feature_text = json.dumps(features.to_dict(), indent=2, ensure_ascii=False)
    
    prompt = f"""???????
```json
{feature_text}
```

?????
- window_loss_pct: ????????? (0-1)
- pad_outage_overlap_hours: ????? R_pad ?????????
- delay_increase_minutes: ??????????
- pad_pressure: R_pad ?????/?????
- slack_min_minutes: ?? slack????
- resource_conflict_pressure: ???????R3/R4?
- num_urgent_tasks: ?????
- trend_window_loss: ?????????
- trend_pad_pressure: Pad ???????
- trend_slack_min_minutes: ?? slack ?????
- trend_delay_increase_minutes: ?????????
- volatility_pad_pressure: Pad ?????

?????????????????????????
- ???????w_delay=50, w_shift=0, w_switch=180, freeze_horizon=0
- window_loss_pct ? ? trend_window_loss > 0 -> ?????w_shift?, w_switch?, freeze_horizon?
- volatility_pad_pressure ? -> ?????w_shift?, w_switch?, freeze_horizon?
- pad_outage_overlap_hours ? -> ?????w_switch?????? 60?
- num_urgent_tasks ? ? delay_increase_minutes ? -> ?????w_delay?????? w_shift ??freeze_horizon ?
- trend_slack_min_minutes ????????-> ?????w_delay??????? freeze_horizon
- trend_pad_pressure ???? -> ??????freeze_horizon?

??? JSON??????/????"""
    
    return prompt


# ============================================================================
# Mock LLM ç­–ç•¥ (ç¡®å®šæ€§ï¼Œç”¨äºç¦»çº¿å¤ç°)
# ============================================================================

class MockLLMPolicy(BasePolicy):
    """
    æ¨¡æ‹Ÿ LLM ç­–ç•¥ - æ ¹æ®ç‰¹å¾ç¡®å®šæ€§è¾“å‡?JSON
    
    è¡Œä¸ºï¼?
    - æ¥æ”¶çŠ¶æ€ç‰¹å¾ä½œä¸ºè¾“å…?
    - æ ¹æ®è§„åˆ™ç¡®å®šæ€§è¾“å‡ºå…ƒå‚æ•° JSONï¼ˆç­‰ä»·äº temperature=0ï¼?
    - ç»è¿‡ JSON schema æ ¡éªŒ
    - æ ¡éªŒå¤±è´¥æ—?fallback åˆ°å›ºå®šå‚æ•?
    - è®°å½•æ¯æ¬¡å†³ç­–çš„è¾“å…¥ç‰¹å¾å’Œè¾“å‡º JSON
    
    ç”¨é€”ï¼š
    - ç¦»çº¿å¤ç°å®éªŒ
    - Baseline å¯¹æ¯”
    """
    
    def __init__(
        self,
        policy_name: str = "mockllm",
        log_dir: Optional[str] = None,
        enable_logging: bool = True,
        episode_id: str = ""
    ):
        """
        Args:
            policy_name: ç­–ç•¥åç§°
            log_dir: æ—¥å¿—ç›®å½•
            enable_logging: æ˜¯å¦è®°å½•æ—¥å¿—
            episode_id: Episode æ ‡è¯†
        """
        self._policy_name = policy_name
        self._enable_logging = enable_logging
        self._log_dir = log_dir
        self._logger = LLMPolicyLogger(log_dir, episode_id) if enable_logging else None
        
        # çŠ¶æ€è¿½è¸?
        self._prev_window_slots: Optional[Dict[str, Set[int]]] = None
        self._recent_shifts = 0
        self._recent_switches = 0
        
        # Fallback å‚æ•°
        self._fallback_params = MetaParams(
            w_delay=50.0,
            w_shift=0.0,
            w_switch=180.0,
            freeze_horizon=0
        )
        
        # ç»Ÿè®¡
        self._call_count = 0
        self._fallback_count = 0
        self._invalid_json_count = 0
    
    @property
    def name(self) -> str:
        return self._policy_name
    
    def set_episode_id(self, episode_id: str) -> None:
        """è®¾ç½® episode ID"""
        if self._logger:
            self._logger.set_episode_id(episode_id)
    
    def decide(
        self,
        state: SimulationState,
        now: int,
        config: Config
    ) -> Tuple[MetaParams, None]:
        """æ ¹æ®ç‰¹å¾å†³ç­–å…ƒå‚æ•?""
        self._call_count += 1
        
        # 1. è®¡ç®—ç‰¹å¾
        features, curr_window_slots = _compute_state_features_for_policy(
            state=state,
            now=now,
            config=config,
            prev_window_slots=self._prev_window_slots,
            recent_shifts=self._recent_shifts,
            recent_switches=self._recent_switches
        )
        
        # æ›´æ–°çŠ¶æ€?
        self._prev_window_slots = curr_window_slots
        
        # 2. ç”Ÿæˆ JSONï¼ˆæ¨¡æ‹?LLM è¾“å‡ºï¼?
        raw_json = self._generate_mock_json(features)
        
        # 3. æ ¡éªŒ JSON
        validation = validate_meta_params_json(raw_json)
        
        # 4. ç¡®å®šæœ€ç»ˆå‚æ•?
        fallback_used = False
        fallback_reason: Optional[str] = None
        
        if validation.is_valid:
            final_params = json_to_meta_params(validation.params)
        else:
            final_params = self._fallback_params
            fallback_used = True
            fallback_reason = "; ".join(validation.errors)
            self._fallback_count += 1
            self._invalid_json_count += 1
        
        # 5. è®°å½•æ—¥å¿—
        if self._logger:
            log = self._logger.log_decision(
                t_now=now,
                features=features,
                raw_output=raw_json,
                extraction_method=validation.extraction_method,
                validation_result=validation,
                llm_cache_hit=True,  # Mock å§‹ç»ˆç­‰ä»·äºç¼“å­˜å‘½ä¸?
                llm_latency_ms=0,
                usage_tokens=0,
                fallback_used=fallback_used,
                fallback_reason=fallback_reason,
                final_params=final_params
            )
            
            # å®æ—¶è¿½åŠ æ—¥å¿—
            if self._log_dir:
                log_file = os.path.join(self._log_dir, "llm_decisions.jsonl")
                self._logger.append_log(log_file, log)
        
        return final_params, None
    
    def _generate_mock_json(self, features: StateFeatures) -> str:
        """
        æ ¹æ®ç‰¹å¾ç”Ÿæˆå…ƒå‚æ•?JSONï¼ˆç¡®å®šæ€§è§„åˆ™ï¼‰
        """
        # åŸºç¡€å‚æ•°
        w_delay = 10.0
        w_shift = 1.0
        w_switch = 5.0
        freeze_horizon = 12
        
        # ¹æÔò 1: ´°¿ÚËğÊ§»òÇ÷ÊÆ¶ñ»¯ -> ÔöÇ¿ÎÈ¶¨
        if features.window_loss_pct > 0.3 or features.trend_window_loss > 0.05:
            freeze_horizon = min(24, freeze_horizon + 6)
            w_shift = min(5.0, w_shift * 2)
        # ¹æÔò 2: Pad ²»¿ÉÓÃÊ±³¤¸ß -> ÔÊĞíÇĞ»»
        if features.pad_outage_overlap_hours > 1.0:
            w_switch = max(1.0, w_switch - 2.0)
        # ¹æÔò 3: ½ô¼±ÈÎÎñ»òÑÓÎóÉı¸ß -> Ìá¸ßÊ±Ğ§
        if features.num_urgent_tasks > 3 or features.delay_increase_minutes > 60:
            w_delay = min(30.0, w_delay + features.num_urgent_tasks * 2)
        # ¹æÔò 4: Slack ÏÂ½µÇ÷ÊÆÃ÷ÏÔ -> Ìá¸ßÊ±Ğ§
        if features.trend_slack_min_minutes < -10:
            w_delay = min(40.0, w_delay + 5.0)
        # ¹æÔò 5: Pad Ñ¹Á¦²¨¶¯´ó -> ÎÈ¶¨ÓÅÏÈ
        if features.volatility_pad_pressure > 0.15:
            w_shift = min(5.0, w_shift + 1.0)
            w_switch = min(10.0, w_switch + 2.0)
            freeze_horizon = min(24, freeze_horizon + 3)
        # ç”Ÿæˆ JSON
        params = {
            "w_delay": round(w_delay, 2),
            "w_shift": round(w_shift, 2),
            "w_switch": round(w_switch, 2),
            "freeze_horizon": freeze_horizon
        }
        
        return json.dumps(params, indent=2)
    
    def reset(self) -> None:
        """é‡ç½®ç­–ç•¥çŠ¶æ€?""
        self._prev_window_slots = None
        self._recent_shifts = 0
        self._recent_switches = 0
        self._call_count = 0
        self._fallback_count = 0
        self._invalid_json_count = 0
        if self._logger:
            self._logger.clear()
    
    def save_logs(self, filepath: str) -> None:
        """ä¿å­˜æ—¥å¿—"""
        if self._logger:
            self._logger.save_logs(filepath)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "policy_name": self._policy_name,
            "call_count": self._call_count,
            "fallback_count": self._fallback_count,
            "invalid_json_count": self._invalid_json_count,
            "fallback_rate": self._fallback_count / self._call_count if self._call_count > 0 else 0.0,
            "cache_hit_count": self._call_count,  # Mock å…¨éƒ¨ç®—ç¼“å­˜å‘½ä¸?
            "total_tokens": 0,
            "total_latency_ms": 0
        }
    
    def get_llm_stats(self) -> Dict[str, Any]:
        """è·å– LLM è°ƒç”¨ç»Ÿè®¡ï¼ˆä¸ RealLLMPolicy æ¥å£ä¸€è‡´ï¼‰"""
        return self.get_stats()
    
    def __repr__(self) -> str:
        return f"MockLLMPolicy(name={self.name})"


# ============================================================================
# çœŸå® LLM ç­–ç•¥ï¼ˆè°ƒç”?Qwen3-32Bï¼?
# ============================================================================

class RealLLMPolicy(BasePolicy):
    """
    çœŸå® LLM ç­–ç•¥ - è°ƒç”¨ Qwen3-32B API
    
    ç‰¹ç‚¹ï¼?
    - åœ?é‡æ’æ£€æŸ¥ç‚¹"è°ƒç”¨ LLM
    - è¾“å…¥ä¸ºç‰¹å¾æ‘˜è¦ï¼ˆæ•°å€¼ç‰¹å¾ï¼‰
    - è¾“å‡ºä¸ºå…ƒå‚æ•° JSON
    - ä¸¥ç¦ï¼šè¾“å‡ºä»»åŠ¡çº§æ’ç¨‹
    - å¤±è´¥æ—?fallback åˆ°å›ºå®šå‚æ•?
    - å®Œæ•´æ—¥å¿—è®°å½•
    """
    
    def __init__(
        self,
        llm_config: Optional["LLMConfig"] = None,
        policy_name: str = "qwen3",
        log_dir: Optional[str] = None,
        enable_logging: bool = True,
        episode_id: str = "",
        fallback_params: Optional[MetaParams] = None
    ):
        """
        Args:
            llm_config: LLM å®¢æˆ·ç«¯é…ç½?
            policy_name: ç­–ç•¥åç§°
            log_dir: æ—¥å¿—ç›®å½•
            enable_logging: æ˜¯å¦è®°å½•æ—¥å¿—
            episode_id: Episode æ ‡è¯†
            fallback_params: å¤±è´¥æ—¶çš„å›é€€å‚æ•°
        """
        if not HAS_LLM_CLIENT:
            raise ImportError(
                "RealLLMPolicy éœ€è¦?llm_client æ¨¡å—ï¼Œè¯·ç¡®ä¿ llm_client.py å­˜åœ¨"
            )
        
        self._policy_name = policy_name
        self._enable_logging = enable_logging
        self._log_dir = log_dir
        self._logger = LLMPolicyLogger(log_dir, episode_id) if enable_logging else None
        
        # LLM å®¢æˆ·ç«¯é…ç½?
        if llm_config is None:
            llm_config = LLMConfig(
                cache_dir=os.path.join(log_dir, "llm_cache") if log_dir else None,
                log_file=os.path.join(log_dir, "llm_raw_calls.jsonl") if log_dir else None
            )
        
        self._llm_config = llm_config
        self._llm_client: Optional[LLMClient] = None  # å»¶è¿Ÿåˆå§‹åŒ?
        
        # çŠ¶æ€è¿½è¸?
        self._prev_window_slots: Optional[Dict[str, Set[int]]] = None
        self._recent_shifts = 0
        self._recent_switches = 0
        
        # Fallback å‚æ•°
        self._fallback_params = fallback_params or MetaParams(
            w_delay=50.0,
            w_shift=0.0,
            w_switch=180.0,
            freeze_horizon=0
        )
        
        # ç»Ÿè®¡
        self._call_count = 0
        self._fallback_count = 0
        self._invalid_json_count = 0
        self._cache_hit_count = 0
        self._total_tokens = 0
        self._total_latency_ms = 0
    
    def _ensure_client(self) -> "LLMClient":
        """ç¡®ä¿ LLM å®¢æˆ·ç«¯å·²åˆå§‹åŒ?""
        if self._llm_client is None:
            self._llm_client = LLMClient(self._llm_config)
        return self._llm_client
    
    @property
    def name(self) -> str:
        return self._policy_name
    
    def set_episode_id(self, episode_id: str) -> None:
        """è®¾ç½® episode ID"""
        if self._logger:
            self._logger.set_episode_id(episode_id)
    
    def decide(
        self,
        state: SimulationState,
        now: int,
        config: Config
    ) -> Tuple[MetaParams, None]:
        """
        åœ¨é‡æ’æ£€æŸ¥ç‚¹è°ƒç”¨ LLM å†³ç­–
        
        æµç¨‹ï¼?
        1. è®¡ç®—çŠ¶æ€ç‰¹å¾?
        2. æ„å»º Prompt
        3. è°ƒç”¨ LLMï¼ˆå¸¦ç¼“å­˜/é‡è¯•ï¼?
        4. è§£æ + Schema æ ¡éªŒ
        5. å¤±è´¥åˆ?Fallback
        6. è®°å½•æ—¥å¿—
        """
        self._call_count += 1
        
        # 1. è®¡ç®—ç‰¹å¾
        features, curr_window_slots = _compute_state_features_for_policy(
            state=state,
            now=now,
            config=config,
            prev_window_slots=self._prev_window_slots,
            recent_shifts=self._recent_shifts,
            recent_switches=self._recent_switches
        )
        
        # æ›´æ–°çŠ¶æ€?
        self._prev_window_slots = curr_window_slots
        
        # 2. è°ƒç”¨ LLM
        client = self._ensure_client()
        user_prompt = build_user_prompt(features)
        
        llm_result = client.call(
            messages=[{"role": "user", "content": user_prompt}],
            system_prompt=SYSTEM_PROMPT
        )
        
        # æ›´æ–°ç»Ÿè®¡
        self._total_latency_ms += llm_result.latency_ms
        self._total_tokens += llm_result.tokens_total
        if llm_result.cache_hit:
            self._cache_hit_count += 1
        
        # 3. å¤„ç† LLM ç»“æœ
        fallback_used = False
        fallback_reason: Optional[str] = None
        
        if not llm_result.success:
            # LLM è°ƒç”¨å¤±è´¥
            fallback_used = True
            fallback_reason = f"LLMè°ƒç”¨å¤±è´¥: {llm_result.error_type}: {llm_result.error_message}"
            self._fallback_count += 1
            
            validation = ValidationResult(
                is_valid=False,
                errors=[fallback_reason],
                extraction_method="failed"
            )
            final_params = self._fallback_params
        else:
            # 4. æ ¡éªŒ JSON
            raw_output = llm_result.raw_text or ""
            validation = validate_meta_params_json(raw_output)
            
            if validation.is_valid:
                final_params = json_to_meta_params(validation.params)
            else:
                # æ ¡éªŒå¤±è´¥ï¼Œä½¿ç”?fallback
                fallback_used = True
                fallback_reason = "; ".join(validation.errors)
                self._fallback_count += 1
                self._invalid_json_count += 1
                final_params = self._fallback_params
        
        # 5. è®°å½•æ—¥å¿—
        if self._logger:
            log = self._logger.log_decision(
                t_now=now,
                features=features,
                raw_output=llm_result.raw_text or "",
                extraction_method=validation.extraction_method,
                validation_result=validation,
                llm_cache_hit=llm_result.cache_hit,
                llm_latency_ms=llm_result.latency_ms,
                usage_tokens=llm_result.tokens_total,
                fallback_used=fallback_used,
                fallback_reason=fallback_reason,
                final_params=final_params
            )
            
            # å®æ—¶è¿½åŠ æ—¥å¿—
            if self._log_dir:
                log_file = os.path.join(self._log_dir, "llm_decisions.jsonl")
                self._logger.append_log(log_file, log)
        
        return final_params, None
    
    def reset(self) -> None:
        """é‡ç½®ç­–ç•¥çŠ¶æ€?""
        self._prev_window_slots = None
        self._recent_shifts = 0
        self._recent_switches = 0
        self._call_count = 0
        self._fallback_count = 0
        self._invalid_json_count = 0
        self._cache_hit_count = 0
        self._total_tokens = 0
        self._total_latency_ms = 0
        if self._logger:
            self._logger.clear()
        if self._llm_client:
            self._llm_client.reset_stats()
    
    def save_logs(self, filepath: str) -> None:
        """ä¿å­˜æ—¥å¿—"""
        if self._logger:
            self._logger.save_logs(filepath)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "policy_name": self._policy_name,
            "call_count": self._call_count,
            "fallback_count": self._fallback_count,
            "invalid_json_count": self._invalid_json_count,
            "cache_hit_count": self._cache_hit_count,
            "total_tokens": self._total_tokens,
            "total_latency_ms": self._total_latency_ms,
            "fallback_rate": self._fallback_count / self._call_count if self._call_count > 0 else 0.0,
            "cache_hit_rate": self._cache_hit_count / self._call_count if self._call_count > 0 else 0.0,
            "avg_latency_ms": self._total_latency_ms / self._call_count if self._call_count > 0 else 0.0
        }
    
    def get_llm_stats(self) -> Dict[str, Any]:
        """è·å– LLM è°ƒç”¨ç»Ÿè®¡"""
        stats = self.get_stats()
        
        # åˆå¹¶ LLM å®¢æˆ·ç«¯ç»Ÿè®?
        if self._llm_client:
            client_stats = self._llm_client.get_stats()
            stats["llm_client"] = client_stats
        
        return stats
    
    def __repr__(self) -> str:
        return f"RealLLMPolicy(name={self.name}, model={self._llm_config.model})"


# åˆ«åï¼ˆå…¼å®¹æ—§ä»£ç ï¼?
LLMInterfacePolicy = RealLLMPolicy


# ============================================================================
# ä¾¿æ·åˆ›å»ºå‡½æ•°
# ============================================================================

def create_mock_llm_policy(
    log_dir: Optional[str] = None,
    enable_logging: bool = True,
    episode_id: str = ""
) -> MockLLMPolicy:
    """åˆ›å»º Mock LLM ç­–ç•¥"""
    return MockLLMPolicy(
        policy_name="mockllm",
        log_dir=log_dir,
        enable_logging=enable_logging,
        episode_id=episode_id
    )


def create_real_llm_policy(
    api_key: Optional[str] = None,
    api_key_env: str = "DASHSCOPE_API_KEY",
    base_url: str = "https://api-inference.modelscope.cn/v1",
    model: str = "Qwen/Qwen3-32B",
    log_dir: Optional[str] = None,
    cache_dir: Optional[str] = None,
    policy_name: str = "qwen3",
    episode_id: str = ""
) -> RealLLMPolicy:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºçœŸå®?LLM ç­–ç•¥
    
    Args:
        api_key: API Keyï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        api_key_env: ç¯å¢ƒå˜é‡å?
        base_url: API ç«¯ç‚¹
        model: æ¨¡å‹å?
        log_dir: æ—¥å¿—ç›®å½•
        cache_dir: ç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤ä¸º log_dir/llm_cacheï¼?
        policy_name: ç­–ç•¥åç§°
        episode_id: Episode æ ‡è¯†
    
    Returns:
        RealLLMPolicy
    """
    if not HAS_LLM_CLIENT:
        raise ImportError("éœ€è¦?llm_client æ¨¡å—")
    
    if cache_dir is None and log_dir:
        cache_dir = os.path.join(log_dir, "llm_cache")
    
    llm_config = LLMConfig(
        api_key=api_key or "",
        api_key_env=api_key_env,
        base_url=base_url,
        model=model,
        cache_dir=cache_dir,
        log_file=os.path.join(log_dir, "llm_raw_calls.jsonl") if log_dir else None,
        temperature=0.0,
        max_tokens=256,
        enable_thinking=False
    )
    
    return RealLLMPolicy(
        llm_config=llm_config,
        policy_name=policy_name,
        log_dir=log_dir,
        enable_logging=True,
        episode_id=episode_id
    )


# ============================================================================
# æ¨¡å—æµ‹è¯•
# ============================================================================

if __name__ == "__main__":
    print("=== LLM Meta Policy Test ===\n")
    
    # æµ‹è¯• JSON æ ¡éªŒ
    print("1. JSON æ ¡éªŒæµ‹è¯•")
    
    test_cases = [
        ('{"w_delay": 15.0, "w_shift": 2.0, "w_switch": 8.0, "freeze_horizon": 18}', True),
        ('```json\n{"w_delay": 20.0, "w_shift": 1.0, "w_switch": 5.0}\n```', True),
        ('thinking...\n{"w_delay": 10, "w_shift": 1, "w_switch": 5}\n\ndone', True),
        ('{"w_delay": 200, "w_shift": -5, "w_switch": 10, "freeze_horizon": 100}', True),  # ä¼šæˆªæ–?
        ('{"w_delay": 10}', False),  # ç¼ºå°‘å¿…éœ€å­—æ®µ
        ('{invalid json}', False),
    ]
    
    for text, should_pass in test_cases:
        result = validate_meta_params_json(text)
        status = "âœ? if result.is_valid == should_pass else "âœ?
        print(f"  {status} valid={result.is_valid}, method={result.extraction_method}: {text[:50]}...")
        if result.warnings:
            print(f"      warnings: {result.warnings[:2]}")
        if result.errors:
            print(f"      errors: {result.errors[:2]}")
    
    print("\n2. MockLLMPolicy æµ‹è¯•")
    
    # åˆ›å»ºæ¨¡æ‹Ÿç‰¹å¾
    features = StateFeatures(
        window_loss_pct=0.4,
        pad_outage_overlap_hours=2.0,
        delay_increase_minutes=30,
        pad_pressure=0.9,
        slack_min_minutes=45,
        resource_conflict_pressure=0.3,
        trend_window_loss=0.05,
        trend_pad_pressure=0.08,
        trend_slack_min_minutes=-12,
        trend_delay_increase_minutes=5,
        volatility_pad_pressure=0.12,
        num_urgent_tasks=5
    )
    )
    
    policy = MockLLMPolicy(enable_logging=False)
    raw_json = policy._generate_mock_json(features)
    print(f"  ç”Ÿæˆçš?JSON:\n{raw_json}")
    
    # æ ¡éªŒç”Ÿæˆçš?JSON
    validation = validate_meta_params_json(raw_json)
    print(f"  æ ¡éªŒç»“æœ: valid={validation.is_valid}, params={validation.params}")
    
    print("\nâœ?æµ‹è¯•å®Œæˆ")
